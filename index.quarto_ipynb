{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: Detecting abnormal attendance patterns from daily data\n",
        "subtitle: A technical presentation - 14th March 2025\n",
        "title-slide-attributes:\n",
        "  data-background-position: top\n",
        "  data-background-image: images/sanctuary-buildings.jpg\n",
        "  data-background-size: cover\n",
        "  data-background-opacity: \"0.1\"\n",
        "author: \"Finn Trinci\"\n",
        "institute: Department for Education\n",
        "format:\n",
        "  revealjs:\n",
        "    theme: [dark, dfe-quarto-slides.scss]\n",
        "    width: 120%\n",
        "    height: 120%\n",
        "    logo: images/DfE_logo_landscape.svg\n",
        "    footer: \"Attendance Analysis\"\n",
        "execute: \n",
        "  echo: true\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Project Overview:\n",
        "\n",
        "**Policy customers**: school accountability.\n",
        "\n",
        "- Wanted a model to flag suspicious attendance data activity to identify, for example, whether a school had an unusual attendance pattern on an Ofsted visit day. \n",
        "\n",
        "**Data experts**: daily attendance data modelers.\n",
        "\n",
        "- Description of the data model and its features such as retrospective data updates.\n",
        "\n",
        "### Tools used:\n",
        "\n",
        "- Databricks for data cleaning and data analysis\n",
        "  - Allows the use of SQL and R scripts seamlessly in the same pipeline, as well as using Spark which optimises queries and distributes compute, efficiently processing the large dataset.\n",
        "- Git for version control, repository hosted on Azure DevOps\n",
        "  - Version control, particularly the use of branches in Git, allows for development and testing while keeping the main project safe. I used `git merge` when combining branches to retain the commit history of development branches.\n",
        "\n",
        "## Starting off\n",
        "\n",
        "- Understanding the daily attendance data model\n",
        "- **Exploratory data analysis** \n",
        "  - Small number of schools with all pupil characteristics unknown e.g. SEN.\n",
        "- Data cleaning - large dataset so investigative\n",
        "\n",
        "\n",
        "There were duplicate entries for the same pupils. Discussions with data modellers revealed that some schools created new learner IDs when a pupil changed year groups, so we took the most recent learner ID.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "``` {sql, eval=FALSE}\n",
        "create or replace temporary view df_year_groups_deduped as\n",
        "select a.*\n",
        "from (\n",
        "  select *, row_number() over (partition by source_learner_id order by admission_date_sk desc) as rn\n",
        "  from df_year_groups\n",
        ") as a\n",
        "where a.rn = 1;\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "- Issues were communicated with the policy team so they could understand the data limitations early.\n",
        "\n",
        "## Methodology: Entropy Analysis\n",
        "\n",
        "Entropy measures the likelihood of certain attendance codes occurring on a given day, with **less likely** codes having **higher entropy**. \n",
        "\n",
        "<br>\n",
        "\n",
        "Calculated as Shannon's entropy:\n",
        "\n",
        "\\begin{gather*}\n",
        "H(X_{j,t}) = -\\sum_{i = 0}^np(x_{i, j, t})\\log_2p(x_{i, j, t})\n",
        "\\end{gather*}\n",
        "\n",
        "Total entropy in school $j$ for session $t$ is equal to the sum for all **$n$ pupils** of the pupil's probability of having attendance code $x_{i,j,t}$ times by the log of that probability. \n",
        "\n",
        "How do we calculate the probability of each code?\n",
        "\n",
        "- Code probabilities are modeled as a **Dirichlet distributed** random variable such that probabilities sum to 1 on any given session.\n",
        "\n",
        "- Probabilities are estimated using **Bayesian updating** on a **pupil level**, so a pupil's most common code has the highest probability. \n",
        "  - The most recent attendance code has the highest weight because codes are persistent across sessions.\n",
        "  - A pupil's attendance in the previous academic year provides the baseline probability\n",
        "\n",
        "## Methodology: Entropy Analysis\n",
        "\n",
        ":::: {.columns}\n",
        "::: {.column width=\"69%\"}\n",
        "Drawbacks of this methodology:\n",
        "\n",
        "  - Processing load: Intensive as we re-calculate a pupil's code probability each session -- with ~9 million pupils that's 18 million sessions per day!\n",
        "    - Originally calculated probabilities using an R package `rdirchlet` but this calculation was done manually in spark SQL to allow for **efficient querying** using spark's distributed processing. \n",
        "    \n",
        "    <br>\n",
        "    \n",
        "    - Created a function to run the model for specific schools rather than all schools, so we could test our outputs (shown on next slide). This meant we had a minimally viable product to show stakeholders the outputs, and explain the drawbacks. \n",
        "    \n",
        ":::\n",
        "::: {.column width=30%}\n",
        "<br>\n",
        "<br>\n",
        "![](images/white_spark-removebg.png)\n",
        "``` {.r code-line-numbers=\"1-5\"}\n",
        "plot_school_entropy(\n",
        "  URN = \"XXXXXXX\",\n",
        "  time_from = as.Date(\"2024-09-01\"),\n",
        "  time_to = as.Date(\"2024-11-24\"),\n",
        "  code_breakdown = TRUE)\n",
        "```\n",
        "\n",
        ":::\n",
        "::::\n",
        "\n",
        "\n",
        "\n",
        "## Entropy Methodology\n",
        "\n",
        "::::{.columns}\n",
        ":::{.column width=42%}\n",
        "Example of an entropy output for one school broken down by specific attendance codes.\n",
        "\n",
        "<br>\n",
        "\n",
        "- Outputs need domain knowledge to be interpretable.\n",
        "- Creates a significant number of false positives.\n",
        "- The majority of entropy is coming from the most common attendance code, uninformative. \n",
        "- Assigned probabilities are crude, need a more sophisticated methodology like a neural network to more accurately predict attendance codes.\n",
        "\n",
        ":::\n",
        ":::{.column width=54%}\n",
        "::::: {style=\"color: #000; background-color: #fff; padding: 10px; border: solid; border-color: #000; text-align: left;\"}\n",
        "![](images/m1_school_entropy_void.png)\n",
        ":::::\n",
        ":::\n",
        "::::\n",
        "\n",
        "## New Methodology: \n",
        "\n",
        "- Presented the limitations of the entropy model and explained the additional resource requirement to get more reliable results with less false positives.\n",
        "\n",
        "<br> \n",
        "\n",
        "- Proposed an **alternative methodology** based on their initial requirements.\n",
        "  - Looks at attendnace on **specific days** instead of every day for every school.\n",
        "  - This means in their Ofsted example, we can look at attendance on the Ofsted inspection date.\n",
        "  - The model is simplified to look at attendance and absence of specific pupil groups.\n",
        "  \n",
        "  \n",
        "## Results of the adjusted methodology:\n",
        "\n",
        "::::{.columns}\n",
        ":::{.column width=51%}\n",
        "\n",
        "The adjusted model flagged schools for further analysis after ofsted visits, and raised significantly less false positives. Benefits:\n",
        "- Less computationally expensive\n",
        "- More interpretable results\n",
        "- Relevant to the current policy climate (this was during the time when Ofsted visits were being reformed)\n",
        "- Code was flexible and reproducible so could be applied in other contexts e.g. looking at attendance on census days.\n",
        "\n",
        ":::\n",
        ":::{.column width=48%}\n",
        "![](images/m2_school_ofsted_dip.jpg)\n",
        ":::\n",
        "::::\n",
        "\n",
        "## Lessons learned\n",
        "\n",
        "- Don't let efforts go to waste! \n",
        "  - The initial work was used as further evidence for investment in a neural network to predict pupil attendance codes. We currently have a contractor working on this and after 3-months the neural network has >80% accuracy!\n",
        "  \n",
        "- Noting users' requirements early is crucial, it made presenting a new methodology possible. \n",
        "  - Similarly, identifying a minimally viable product is important in case of future road blocks.\n",
        "\n",
        "- It's okay for projects to pivot as long as the reasons are communicated early and effectively. Always come prepared with potential solutions!\n",
        "\n",
        "\n",
        "::: center-content\n",
        "## Any questions?\n",
        "\n",
        "Slides built from the DfE analytical services' [Quarto template](https://github.com/dfe-analytical-services/quarto-slides-template)\n",
        ":::"
      ],
      "id": "868ba53b"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}